{"cells":[{"cell_type":"markdown","metadata":{"id":"jG7ZEc_982io"},"source":["# StyleGAN2-ADA-PyTorch\n","\n","## Preface\n","This StyleGAN2-ADA-PyTorch repository (including this Colab notebook) was forked from [Derrick Schultz](https://github.com/dvschultz/stylegan2-ada-pytorch), which was forked from [Nvidia's original repo](https://github.com/NVlabs/stylegan2-ada-pytorch). Additional comments and instructions added by Doug Rosman.\n","\n","Last Updated: March 2024"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":590,"status":"ok","timestamp":1678219889121,"user":{"displayName":"Doug Rosman","userId":"12469472491464808109"},"user_tz":360},"id":"SO_YKcaT-NBq","outputId":"bae22ae9-fa99-49c5-b2c4-83317052125e"},"outputs":[],"source":["# Required\n","# Connect to GPU instance\n","\n","!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n350xrkcA2vf"},"outputs":[],"source":["# Required\n","# Install dependencies\n","!pip install \"jax[cuda11_cudnn805]==0.3.10\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install timm==0.4.12 ftfy==6.1.1 ninja==1.10.2 opensimplex\n","!pip install tqdm\n","!pip install imageio-ffmpeg\n","!pip install click\n","!pip install requests\n","!pip install scipy\n","!pip install psutil"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mi5r1m4uR-xf"},"source":["## Find a pre-trained model"]},{"cell_type":"markdown","metadata":{"id":"PqgtDzuuSD8t"},"source":["- Training models from scratch takes a lot of time, so it's best to transfer learn from a pretrained model.\n","- Download any of the models below, and then upload the .pkl file to the `pretrained` folder in the repo.\n","- The dimensions of your images must match the dimensions of the pretrained model (1024x1024, 512x512, 256x256)\n","- The pretrained model you start with will have a subtle influence on the way your images look.\n","\n","**Lists of pretrained models to choose from**\n","- https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ (Nvidia's official models (ada version))\n","  - [FFHQ](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl) (1024) (Flickr Faces High Quality, realistic human faces)\n","  - [MetFaces](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl) (1024) (Faces from paintings in the Met Museum of Art Collection)\n","  - [AFHQ Wild](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqwild.pkl) (512) (Animal Faces High Quality, realistic wild animal faces)\n","  - [AFHQ Cat](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl) (512) (Animal Faces High Quality, realistic cat faces)\n","  - [AFHQ Dog](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqdog.pkl) (512) (Animal Faces High Quality, realistic dog faces)\n","  - [BreCaHAD](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/brecahad.pkl) (512) (Breast Cancer microscopic biopsy images)\n","\n","- https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/ (Nvidia's official models)\n","  - [Cat](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl) (256)\n","  - [Church](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-church-config-f.pkl) (256)\n","  - [Horse](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-horse-config-f.pkl) (256)\n","\n","- https://github.com/justinpinkney/awesome-pretrained-stylegan2 (collection by Justin Pinkney. There are more there than the few listed below)\n","  - [FFHQ 512](https://mega.nz/#!eQdHkShY!8wyNKs343L7YUjwXlEg3cWjqK2g2EAIdYz5xbkPy3ng) (512) (Lower resolution version of the FFHQ model)\n","  - [Flowers](https://drive.google.com/uc?id=13onBTt6xVwKmYTRCFbONBTbSXFqeiVcK) (256)\n","  - [Anime Portraits](https://mega.nz/#!PeIi2ayb!xoRtjTXyXuvgDxSsSMn-cOh-Zux9493zqdxwVMaAzp4) (512)"]},{"cell_type":"markdown","metadata":{"id":"bqsU3uta6jzo"},"source":["## Upload your dataset"]},{"cell_type":"markdown","metadata":{"id":"OdncQC9FInep"},"source":["- Before uploading, put all your images in a folder and compress the folder to a .zip file.\n","- Upload your zip file to the __datasets__ folder\n","\n","__Zipping up your folders__\n","\n","__Windows Users:__\n","1. Right-click the folder and select **Send to** --> **Compressed (zipped) folder**\n","\n","__Mac Users:__\n","\n","Zipping files on a Mac with the native compression tool creates a file called \".DS_Store\" inside the folder, which causes some code not to work. I recommend installing a free piece of software called __Keka__.\n","\n","1. Download and install Keka ([click to download](https://d.keka.io/))\n","1. Open Keka\n","1. Make sure the box next to **'Exclude Mac resource forks'** is checked\n","1. Drag and drop your dataset folder directly onto the open Keka app\n","1. It may ask you where to save your file. Select your desintation and compress your folder\n","1. Upload the .zip file to the **datasets** folder\n","\n","![good zip with Keka](https://raw.githubusercontent.com/dougrosman/stylegan2-ada-pytorch/main/images/keka.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IB1t-nTSItRZ"},"source":["## Preparing your images for training (batch cropping + resizing)"]},{"cell_type":"markdown","metadata":{"id":"mQc-JnFPmQLe"},"source":["In order to train, all your images\n","- must be the exact same dimensions\n","- must be square, either 256x256, 512x512 or 1024x1024, depending on the pretrained model you're using\n","- must be RGB images.\n","\n","The following script resizes, crops, and converts your images to RGB."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ELqFOOtWB-P3"},"outputs":[],"source":["# Set your variables\n","\n","width = 512  # EDIT THIS to your desired dimension (256, 512 or 1024)\n","height = width\n","dataset = \"dataset_name\" # EDIT THIS to the name of your data set file (without the .zip)\n","\n","data_src = \"./datasets/\" + dataset + \".zip\"\n","data_dest = \"./datasets/\" + dataset + \"-\" + str(width) + \".zip\" "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFa0yeGaDkRO"},"outputs":[],"source":["# Run this after setting your variables to process your images\n","\n","!python dataset_tool-jpg.py --source=$data_src --dest=$data_dest --width=$width --height=$height --transform=\"center-crop\""]},{"cell_type":"markdown","metadata":{"id":"5B-h6FpB9FaK"},"source":["## Training your model"]},{"cell_type":"markdown","metadata":{"id":"LCibYLXYPHPg"},"source":["### Setting your variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EXUik2DJtrg"},"outputs":[],"source":["# Edit the following variables to make sure you're training with the correct settings.\n","\n","# Required\n","# The name of your data set file (without the .zip)\n","dataset = \"dataset_name\"\n","\n","# The size of your images (256, 512 or 1024)\n","size = 512 \n","\n","# the .pkl file you're transfer learning from. This file must be in the \"pretrained\" folder\n","resume_from = \"afhq_wild.pkl\"\n","\n","# (OPTIONAL)\n","# set how frequently you save checkpoints. Lower == more frequent, but takes up more storage space\n","snapshot_count = 2\n","\n","# set to 'True' to double your data set size by duplicating and mirroring each image horizontally. If your dataset includes text, or images that must face a certain direction, keep this set to false.\n","mirror_x = False \n","\n","\n","# (DON'T CHANGE THESE UNLESS YOU KNOW WHAT YOU'RE DOING)\n","gamma_value = 50.0\n","augs = 'bg'\n","config = '11gb-gpu'\n","\n","####\n","dataset_path = \"./datasets/\" + dataset + \"-\" + str(size) + \".zip\"\n","output_dir = \"./results\"\n","resume_from = \"./pretrained/\" + resume_from\n","aug_strength = 0\n","train_count = 0 "]},{"cell_type":"markdown","metadata":{"id":"aGAdtpujMs6s"},"source":["### Setting your variables for resuming training (optional)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaOvn472MJk9"},"outputs":[],"source":["# Set these variables after running the initial \"setting your variables\" cell.\n","\n","# From the file explorer, right click on your pickle file and select \"copy path\", then paste that below\n","resume_from = 'paste filepath here'\n","\n","# This number should match the number at the end of your pickle file (e.g. if you're resuming from network-snapshot-000200.pkl, you would set train_count to 200.\n","train_count = 0  \n","\n","# You can find the aug_strength in the \"log.txt\" file of the training that produced the pickle file you're be resuming from. Scroll down to the bottom of log.txt. Select the aug strength that corresponds with the kimg number of your train_count above.\n","aug_strength = 0.000 "]},{"cell_type":"markdown","metadata":{"id":"Sb1s2hDhSOsf"},"source":["### Start Training\n"]},{"cell_type":"markdown","metadata":{"id":"ZFHDPycXdOv0"},"source":["This is it, the moment we've been waiting for! Once you start your training, remember to keep this tab open and your computer on. Make sure you computer doesn't fall asleep."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vm0zduxRLsvl"},"outputs":[],"source":["# After setting your variables above, run this cell to start your training.\n","!python train.py --gpus=1 --cfg=$config --metrics=None --outdir=$output_dir --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"]},{"cell_type":"markdown","metadata":{"id":"tPpeQzPudaYC"},"source":["## Generating Images and Videos"]},{"cell_type":"markdown","metadata":{"id":"WL2V-Teqoiei"},"source":["For an in-depth explanation of the latent space, interpolations, and the different ways to generate latent space walks, check out this video: [StyleGAN2 In-Depth Week 3 (latent spaces, linear interpolations and noise loops](https://www.youtube.com/watch?v=jKJCv9VGqLQ&t=1998s) by Artificial Images (Derrick Schultz). The link takes you to the part of the video where he begins to go through these. (33:18) "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Key Terms"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vGvgZDjAlRmG"},"source":["- `network` - The pickle file to generate from. Network is another term for \"trained model.\"\n","- `seed` - Our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation. Can be a number between 0 to 4,294,967,295. \n","- `truncation` - How much to \"truncate\" the latent space. A measure of how \"creative\" or \"realistic\" the generator will be. Best results between -1 and 1. Low truncation (close to 0) produces a lower variety of images, adhering to the \"average\" image of your trained model. Higher truncation (close to 1) produces more varied images. Beyond -1 or 1, the results start getting more wonky, as these images are outside the bounds of the latent space.\n","- `interpolation` - Interpolation is a general concept of moving between two things. A trained model is a 512-dimension space, and *interpolating through* this space takes us through a range of images. There are different interpolation methods, each of which creates different \"routes\" through the latent space.\n","- `diameter` - When doing a circular or noise loop, a larger diameter means traversing more of the latent space. A larger diameter leads to faster animations"]},{"cell_type":"markdown","metadata":{"id":"8zoiE_clkGsc"},"source":["### Generate random images\n","\n","Generate random individual images"]},{"cell_type":"markdown","metadata":{"id":"mYdyfH0O8In_"},"source":["**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--trunc`: How much truncation, a number between -1 to 1 will produce the best results, but technically can be infinite. \n","\n","`--seeds`: The seeds to generate images from. Can use a range of values (e.g. 400-500), or comma-separated seed values (e.g. 10, 20, 30).\n","\n","`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYRXenMoZSHf"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/random-1000-1020-noncon \\\n","--trunc=1 \\\n","--seeds=1000-1020 \\\n","--network=./pretrained/cat256.pkl"]},{"cell_type":"markdown","metadata":{"id":"uP1HsU_CPcF5"},"source":["### Generate Noise Loop Animation\n"]},{"cell_type":"markdown","metadata":{"id":"loD7PzbyNqMK"},"source":["If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path through the z space to show you a diverse set of images.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--trunc`: truncation value\n","\n","`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels. (something between 0.5 and 10)\n","\n","`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n","\n","`--network`: path to your .pkl file\n","\n","`--frames`: How many frames to produce\n","\n","`--fps`: frames per second of your video (24, 30, 48, 60)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GmcpoAdJfWN"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/noiseloop_1003_d1 \\\n","--trunc=0.75 \\\n","--process=\"interpolation\" \\\n","--interpolation=\"noiseloop\" \\\n","--diameter=1 \\\n","--random_seed=1000 \\\n","--network=./pretrained/cat256.pkl \\\n","--frames=300 \\\n","--fps=30"]},{"cell_type":"markdown","metadata":{"id":"PkKFb-4CedOq"},"source":["### Generate Circular Loop Animation\n","Compared to the Noise Loop, the circular loop will feel much more even, while still providing a random loop."]},{"cell_type":"markdown","metadata":{"id":"sFoQYfBuO5Bv"},"source":["I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--trunc`: truncation value\n","\n","`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n","\n","`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n","\n","`--network`: path to your .pkl file\n","\n","`--frames`: How many frames to produce\n","\n","`--fps`: frames per second of your video (24, 30, 48, 60, 120, 144)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98U--O29wyB-"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/circularloop_1003_d1000 \\\n","--trunc=0.75 \\\n","--process=\"interpolation\" \\\n","--interpolation=\"circularloop\" \\\n","--diameter=1000 \\\n","--random_seed=1003 \\\n","--network=./pretrained/cat256.pkl \\\n","--frames=120 \\\n","--fps=60"]},{"cell_type":"markdown","metadata":{"id":"VjOTCWVonoVL"},"source":["### Generate Truncation Traversal Animation\n","\n","Take one seed and interpolate between a starting and ending truncation amount.\n"]},{"cell_type":"markdown","metadata":{"id":"5PIuUSgiuPOm"},"source":["Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--start`: Starting truncation value.\n","\n","`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n","\n","`--seeds`: Pass this only one seed. Pick a favorite from your generated images.\n","\n","`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n","\n","`--network`: Again, this should be the path to your .pkl file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyzdGr7OnrMG"},"outputs":[],"source":["!python generate.py \\\n","--process=\"truncation\" \\\n","--outdir=./generated/trunc-1002_1004__-2-2 \\\n","--start=-2 \\\n","--stop=2 \\\n","--increment=0.005 \\\n","--seeds=1002 \\\n","--network=./pretrained/cat256.pkl \\\n","--fps=60"]},{"cell_type":"markdown","metadata":{"id":"OSzj0igO8Lfu"},"source":["### Generate Linear Interpolation (Lerp) Animation\n","A straight line from one point (seed) in the latent space to another. The distance from one seed to another will vary, so some interpolations will be fast, others slow."]},{"cell_type":"markdown","metadata":{"id":"FDLhM9Q8wUp2"},"source":["\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--space`: \"z\" or \"w\". Depending on your model, w-space may interpolate more smoothly (like with faces).\n","\n","`--trunc`: truncation value\n","\n","`--seeds`: list of seeds to interpolate between. Comma separated. Each set of seeds will be given the same number of frames to complete the interpolation. Set the last seed to the same number as the first to create a seamless looping video.\n","\n","`--network`: path to your .pkl file\n","\n","`--frames`: How many frames to produce per interpolation. (e.g. if you set your frames to **30** and your seeds to 10, 20, 30, 10, this will create a video with **90** frames (10 to 20 (30 frames), 20 to 30 (30 frames), 30 to 10 (30 frames))\n","\n","`--fps`: frames per second of your video (24, 30, 48, 60, 120, 144)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqkiskly8S5_"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/lerp-10_20_30_40_10 \\\n","--space=\"w\" \\\n","--trunc=0.75 \\\n","--process=\"interpolation\" \\\n","--seeds=10,20,30,40,10 \\\n","--network=./pretrained/cat256.pkl \\\n","--frames=300 \\\n","--fps=30"]},{"cell_type":"markdown","metadata":{"id":"7CDdorCE65Dd"},"source":["### Generate Projection Animation"]},{"cell_type":"markdown","metadata":{"id":"Br_1vrEoQHoB"},"source":["Project images into the latent space. The network looks at your image and tries to reverse engineer its latent vector in order to re-create your image from what the latent space understands about images.\n","\n","**Steps:**\n","1. Find an image you want to project\n","2. Resize it so that it is the same dimensions as your trained model\n","3. Upload the image to your Google Drive in the stylegan2-ada-pytorch repository (in the generated/project/input folder)\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--target`: Filepath to your input image\n","\n","`--num-steps`: how many iterations the projector should run for. Lower will mean fewer steps and less likelihood of a good projection. Higher will take longer but will likely produce better images.\n","\n","`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziPz6ewz6-h3"},"outputs":[],"source":["!python projector.py \\\n","--outdir=./generated/projection/lea6 \\\n","--target=./generated/projection/input/lea256.jpg \\\n","--num-steps=30 \\\n","--seed=20 \\\n","--network=./pretrained/cat256.pkl \\\n","--video-target-concat=True # set to True for side-by-side of target image + output video"]},{"cell_type":"markdown","metadata":{"id":"IRFNacNH60dl"},"source":["### Align Faces (for projection or for training)"]},{"cell_type":"markdown","metadata":{"id":"M490BUeV64oX"},"source":["*Work in progress, this worfklow is a tad wonky*\n","\n","Align faces using the same process used to align faces for the FFHQ data set. Run this script if you plan to train a data set on faces, or project a face into a latent space with aligned faces like FFHQ, metfaces, or AFHQ (Wild, Dog or Cat) Although this should maybe go in the Data Set Preparation section, I'm including it here at the bottom since it's a more of a niche process that won't be run that frequently."]},{"cell_type":"markdown","metadata":{"id":"5RiMGDfPniUj"},"source":["**Steps**\n","(again, this is a work in progress, so the process is a little less streamlined)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLzBTsxxqCwy"},"outputs":[],"source":["# 1. \"clean\" your images by making sure the alignment script can process them\n","!python util/align_faces/clean_images_downscale.py /path/to/dataset.zip 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3eIharrBq1KK"},"outputs":[],"source":["# 2. unzip your images to a folder called raw_images\n","!unzip util/align_faces/output.zip -d raw_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vadKricrQMVT"},"outputs":[],"source":["# 3. align the faces, saving them to a folder called aligned\n","!python util/align_faces/align_faces.py raw_faces aligned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifKKwOwUPrw-"},"outputs":[],"source":["#4. (optional) zip up the aligned faces so they can be used for training\n","!zip -r util/align_faces/aligned.zip aligned"]}],"metadata":{"colab":{"collapsed_sections":["nTxKm73p6Vv1","W7vv9HtN_tWE","-9DH84cXALLF","VBRt41MUBFbE","mi5r1m4uR-xf","bqsU3uta6jzo","OpHg9Y9DEOrC","IB1t-nTSItRZ","ul5PPscSfIj2","LCibYLXYPHPg","aGAdtpujMs6s","Sb1s2hDhSOsf","tPpeQzPudaYC","8zoiE_clkGsc","VjOTCWVonoVL","OSzj0igO8Lfu","uP1HsU_CPcF5","PkKFb-4CedOq","7CDdorCE65Dd","IRFNacNH60dl","VznRirOE5ENI","N4jgk98lPocO","Yi3d7xzpN2Uj"],"provenance":[{"file_id":"https://github.com/dougrosman/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb","timestamp":1617234210817}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
